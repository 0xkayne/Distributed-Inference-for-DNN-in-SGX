# å¤šå¤´æ³¨æ„åŠ›æ¨¡å—é‡æ„æ€»ç»“

## ğŸ“‹ ä¿®æ”¹æ¦‚è§ˆ

æœ¬æ¬¡é‡æ„å°†æ‰€æœ‰ Transformer æ¶æ„æ¨¡å‹çš„å¤šå¤´æ³¨æ„åŠ›å®ç°ç»Ÿä¸€ä¸ºå¯å¤ç”¨çš„é€šç”¨æ¨¡å—ï¼Œæ”¯æŒæ‰¹é‡è®¡ç®—å’Œ per-head ç»†ç²’åº¦åˆ†æä¸¤ç§æ¨¡å¼ã€‚

## ğŸ¯ ä¿®æ”¹ç›®æ ‡

1. **æ¶ˆé™¤ä»£ç é‡å¤**ï¼š6 ä¸ªæ¨¡å‹æ–‡ä»¶ä¸­æœ‰ç›¸ä¼¼çš„ `MultiHeadSelfAttention` ç±»
2. **ç»Ÿä¸€æ¥å£**ï¼šæä¾›ä¸€è‡´çš„å¤šå¤´æ³¨æ„åŠ›åˆ›å»ºå’Œä½¿ç”¨æ–¹å¼
3. **æ”¯æŒç»†ç²’åº¦åˆ†æ**ï¼šåœ¨ TEE ç¯å¢ƒä¸‹å¯ä»¥å•ç‹¬æµ‹é‡æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æ€§èƒ½
4. **ä¿æŒå…¼å®¹æ€§**ï¼šä¸æ”¹å˜æ¨¡å‹çš„æ•°å­¦è®¡ç®—é€»è¾‘

## ğŸ“ æ–°å¢æ–‡ä»¶

### 1. é€šç”¨æ³¨æ„åŠ›æ¨¡å— (`python/layers/attention/`)

```
python/layers/attention/
â”œâ”€â”€ __init__.py                  # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ base_attention.py            # åŸºç¡€æŠ½è±¡ç±» (BaseMultiHeadAttention)
â”œâ”€â”€ batched_attention.py         # æ‰¹é‡è®¡ç®—æ¨¡å¼ (BatchedMultiHeadAttention)
â”œâ”€â”€ per_head_attention.py        # Per-head è®¡ç®—æ¨¡å¼ (PerHeadMultiHeadAttention)
â”œâ”€â”€ attention_factory.py         # å·¥å‚å‡½æ•° (create_multi_head_attention)
â””â”€â”€ README.md                    # è¯¦ç»†æ–‡æ¡£
```

**æ ¸å¿ƒæ¥å£**ï¼š
```python
from python.layers.attention import create_multi_head_attention

attn = create_multi_head_attention(
    sid=0,
    name_prefix="encoder0_attn",
    enclave_mode=ExecutionModeOptions.Enclave,
    embed_dim=768,
    num_heads=12,
    batch_size=1,
    seq_len=128,
    per_head_mode=False,  # True å¯ç”¨ per-head æ¨¡å¼
    layer_mode_overrides={}
)
```

## ğŸ“ ä¿®æ”¹çš„æ–‡ä»¶

### 1. BERT (`experiments/models/sgx_bert_native.py`)

**å˜æ›´**ï¼š
- âŒ åˆ é™¤ï¼šæ—§çš„ `MultiHeadSelfAttention` ç±»ï¼ˆ~180 è¡Œï¼‰
- âœ… æ·»åŠ ï¼šå¯¼å…¥ `create_multi_head_attention`
- âœ… ä¿®æ”¹ï¼š`BERTEncoderBlock` ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºæ³¨æ„åŠ›
- âœ… æ·»åŠ ï¼š`use_per_head_attention` å‚æ•°æ”¯æŒ

**å…³é”®ä¿®æ”¹**ï¼š
```python
# æ—§ä»£ç 
self.attn = MultiHeadSelfAttention(
    sid, f"{name_prefix}_attn", enclave_mode,
    embed_dim=embed_dim, num_heads=num_heads,
    batch_size=batch_size, seq_len=seq_len,
    layer_mode_overrides=overrides
)
self.layers.extend(self.attn.layers)

# æ–°ä»£ç 
self.attn = create_multi_head_attention(
    sid=sid,
    name_prefix=f"{name_prefix}_attn",
    enclave_mode=enclave_mode,
    embed_dim=embed_dim,
    num_heads=num_heads,
    batch_size=batch_size,
    seq_len=seq_len,
    per_head_mode=use_per_head_attention,
    layer_mode_overrides=overrides
)
self.layers.extend(self.attn.get_all_layers())
```

### 2. ALBERT (`experiments/models/sgx_albert_native.py`)

**å˜æ›´ç±»å‹**ï¼šä¸ BERT ç›¸åŒ
- åˆ é™¤æ—§çš„ `MultiHeadSelfAttention` ç±»
- ä½¿ç”¨ç»Ÿä¸€çš„ `create_multi_head_attention` å·¥å‚
- æ·»åŠ  `use_per_head_attention` å‚æ•°

### 3. DistilBERT (`experiments/models/sgx_distilbert_native.py`)

**å˜æ›´ç±»å‹**ï¼šä¸ BERT ç›¸åŒ
- ä¿æŒ pre-norm æ¶æ„ä¸å˜
- ä½¿ç”¨ç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å—

### 4. TinyBERT (`experiments/models/sgx_tinybert_native.py`)

**å˜æ›´ç±»å‹**ï¼šä¸ BERT ç›¸åŒ
- è½»é‡çº§é…ç½®ä¿æŒä¸å˜
- ä½¿ç”¨ç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å—

### 5. ViT (`experiments/models/sgx_vit_native.py`)

**å˜æ›´ç±»å‹**ï¼šä¸ BERT ç›¸åŒ
- Patch embedding å’Œä½ç½®ç¼–ç ä¿æŒä¸å˜
- æ³¨æ„åŠ›éƒ¨åˆ†ä½¿ç”¨ç»Ÿä¸€æ¨¡å—

**ç‰¹æ®Šä¹‹å¤„**ï¼š
```python
# ViT çš„åºåˆ—é•¿åº¦åŒ…æ‹¬ CLS token
seq_len = num_patches + 1  # 197 for 224x224 images with 16x16 patches
```

### 6. Swin Transformer (`experiments/models/sgx_swin_native.py`)

**å˜æ›´ç±»å‹**ï¼šè¾ƒå¤§æ”¹åŠ¨

**æ—§ä»£ç **ï¼š`WindowAttention` ç±»åŒ…å«ç®€åŒ–çš„æ³¨æ„åŠ›å±‚å®šä¹‰

**æ–°ä»£ç **ï¼š`WindowAttention` å°è£…é€šç”¨æ³¨æ„åŠ›æ¨¡å—
```python
class WindowAttention:
    def __init__(self, ...):
        # å…³é”®ï¼šå°†æ¯ä¸ªçª—å£è§†ä¸ºä¸€ä¸ª batch
        total_window_batches = num_windows * batch_size
        
        # ä½¿ç”¨ç»Ÿä¸€æ³¨æ„åŠ›å·¥å‚
        self.attn = create_multi_head_attention(
            sid=sid,
            name_prefix=name_prefix,
            enclave_mode=enclave_mode,
            embed_dim=dim,
            num_heads=num_heads,
            batch_size=total_window_batches,  # çª—å£æ•°
            seq_len=window_size * window_size,  # 49 for 7x7
            per_head_mode=use_per_head_attention,
            layer_mode_overrides=layer_mode_overrides
        )
    
    def connect(self, prev_layer):
        return self.attn.connect(prev_layer)
```

## ğŸ“Š ä»£ç ç»Ÿè®¡

### å‡å°‘çš„é‡å¤ä»£ç 

| æ–‡ä»¶ | åˆ é™¤è¡Œæ•° | é‡å¤ä»£ç  |
|------|---------|---------|
| sgx_bert_native.py | ~180 | MultiHeadSelfAttention |
| sgx_albert_native.py | ~180 | MultiHeadSelfAttention |
| sgx_distilbert_native.py | ~180 | MultiHeadSelfAttention |
| sgx_tinybert_native.py | ~180 | MultiHeadSelfAttention |
| sgx_vit_native.py | ~180 | MultiHeadSelfAttention |
| sgx_swin_native.py | ~90 | WindowAttention (ç®€åŒ–ç‰ˆ) |
| **æ€»è®¡** | **~990 è¡Œ** | **é‡å¤å®ç°** |

### æ–°å¢çš„é€šç”¨ä»£ç 

| æ–‡ä»¶ | æ–°å¢è¡Œæ•° | åŠŸèƒ½ |
|------|---------|------|
| base_attention.py | ~80 | åŸºç¡€æŠ½è±¡ç±» |
| batched_attention.py | ~210 | æ‰¹é‡æ¨¡å¼å®ç° |
| per_head_attention.py | ~330 | Per-head æ¨¡å¼å®ç° |
| attention_factory.py | ~110 | å·¥å‚å‡½æ•° |
| **æ€»è®¡** | **~730 è¡Œ** | **é€šç”¨å®ç°** |

**å‡€å‡å°‘**ï¼š~260 è¡Œä»£ç ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤šåŠŸèƒ½ï¼ˆper-head æ¨¡å¼ï¼‰

## âœ¨ æ–°åŠŸèƒ½

### 1. Per-Head æ¨¡å¼

**æ‰¹é‡æ¨¡å¼**ï¼ˆåŸæœ‰ï¼‰ï¼š
```
encoder0_attn_q_proj          â†’ 2.34 ms
encoder0_attn_k_proj          â†’ 2.31 ms
encoder0_attn_v_proj          â†’ 2.35 ms
encoder0_attn_qk_matmul       â†’ 5.23 ms  (æ‰€æœ‰12ä¸ªå¤´)
encoder0_attn_attn_softmax    â†’ 1.45 ms
encoder0_attn_attn_v_matmul   â†’ 5.18 ms
encoder0_attn_out_proj        â†’ 2.38 ms
```

**Per-head æ¨¡å¼**ï¼ˆæ–°å¢ï¼‰ï¼š
```
encoder0_attn_q_proj              â†’ 2.34 ms
encoder0_attn_k_proj              â†’ 2.31 ms
encoder0_attn_v_proj              â†’ 2.35 ms
encoder0_attn_head0_qk_matmul     â†’ 0.45 ms  (å•ä¸ªå¤´)
encoder0_attn_head0_softmax       â†’ 0.12 ms
encoder0_attn_head0_attn_v_matmul â†’ 0.43 ms
encoder0_attn_head1_qk_matmul     â†’ 0.46 ms  (å•ä¸ªå¤´)
encoder0_attn_head1_softmax       â†’ 0.13 ms
encoder0_attn_head1_attn_v_matmul â†’ 0.44 ms
... (å…±12ä¸ªå¤´)
encoder0_attn_out_proj            â†’ 2.38 ms
```

### 2. çµæ´»çš„æ¨¡å¼åˆ‡æ¢

```python
# æ‰¹é‡æ¨¡å¼ - ç”¨äºç”Ÿäº§æ¨ç†
model = create_bert_base(
    use_per_head_attention=False  # é»˜è®¤
)

# Per-head æ¨¡å¼ - ç”¨äºæ€§èƒ½åˆ†æ
model = create_bert_base(
    use_per_head_attention=True
)
```

### 3. ç»Ÿä¸€çš„ Swin Window Attention

Swin çš„çª—å£æ³¨æ„åŠ›ç°åœ¨ä¹Ÿä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼Œé€šè¿‡å°†çª—å£è§†ä¸º batch æ¥å®ç°ï¼š

```python
# 64 ä¸ª 7x7 çš„çª—å£
num_windows = 64
window_tokens = 49

# åˆ›å»ºæ³¨æ„åŠ›ï¼šå°†æ¯ä¸ªçª—å£ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ batch
attn = create_multi_head_attention(
    batch_size=num_windows,  # 64
    seq_len=window_tokens,   # 49
    ...
)
```

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### åœ¨ç°æœ‰æ¨¡å‹ä¸­å¯ç”¨ per-head æ¨¡å¼

```python
from experiments.models.sgx_bert_native import create_bert_base
from python.utils.basic_utils import ExecutionModeOptions

# åˆ›å»ºæ¨¡å‹ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
model_batched = create_bert_base(
    num_classes=2,
    enclave_mode=ExecutionModeOptions.Enclave,
    use_per_head_attention=False  # é»˜è®¤
)

# åˆ›å»ºæ¨¡å‹ï¼ˆper-head æ¨¡å¼ï¼‰
model_per_head = create_bert_base(
    num_classes=2,
    enclave_mode=ExecutionModeOptions.Enclave,
    use_per_head_attention=True  # å¯ç”¨ç»†ç²’åº¦åˆ†æ
)

# å±‚æ•°å¯¹æ¯”
print(f"æ‰¹é‡æ¨¡å¼å±‚æ•°: {len(model_batched.get_all_layers())}")
# è¾“å‡º: ~150 å±‚

print(f"Per-head æ¨¡å¼å±‚æ•°: {len(model_per_head.get_all_layers())}")
# è¾“å‡º: ~1500+ å±‚ (æ¯ä¸ªæ³¨æ„åŠ›æ¨¡å—æœ‰ 12 ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—)
```

### Profiling ä½¿ç”¨

```python
# åœ¨ profile_bert_enclave.py ä¸­æ·»åŠ å‚æ•°
profiler = BERTEnclaveProfiler(
    model_variant='base',
    use_per_head_attention=True  # å¯ç”¨ç»†ç²’åº¦åˆ†æ
)
```

## ğŸ“ æŠ€æœ¯äº®ç‚¹

### 1. å•ä¸€èŒè´£åŸåˆ™

- `BaseMultiHeadAttention`: å®šä¹‰æ¥å£
- `BatchedMultiHeadAttention`: æ‰¹é‡è®¡ç®—å®ç°
- `PerHeadMultiHeadAttention`: Per-head å®ç°
- `create_multi_head_attention`: å·¥å‚åˆ›å»º

### 2. å¼€é—­åŸåˆ™

- å¯¹æ‰©å±•å¼€æ”¾ï¼šå¯ä»¥è½»æ¾æ·»åŠ æ–°çš„æ³¨æ„åŠ›æ¨¡å¼
- å¯¹ä¿®æ”¹å°é—­ï¼šç°æœ‰æ¨¡å‹æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨

### 3. ä¾èµ–å€’ç½®åŸåˆ™

- æ‰€æœ‰æ¨¡å‹ä¾èµ–æŠ½è±¡æ¥å£ï¼ˆ`BaseMultiHeadAttention`ï¼‰
- ä¸ä¾èµ–å…·ä½“å®ç°

### 4. DRY åŸåˆ™ï¼ˆDon't Repeat Yourselfï¼‰

- æ¶ˆé™¤äº† 6 ä¸ªæ¨¡å‹ä¸­çš„é‡å¤ä»£ç 
- ç»Ÿä¸€ç»´æŠ¤ç‚¹

## ğŸ§ª æµ‹è¯•å»ºè®®

### 1. åŠŸèƒ½æµ‹è¯•

```bash
# æµ‹è¯•æ‰¹é‡æ¨¡å¼
python experiments/models/sgx_bert_native.py

# æµ‹è¯• per-head æ¨¡å¼
# ï¼ˆéœ€è¦åœ¨ä»£ç ä¸­ä¸´æ—¶è®¾ç½® use_per_head_attention=Trueï¼‰
```

### 2. Profiling æµ‹è¯•

```bash
# Enclave profiling (æ‰¹é‡æ¨¡å¼)
LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6 \
python -m experiments.models.profile_bert_enclave --model base

# Enclave profiling (per-head æ¨¡å¼)
# ï¼ˆéœ€è¦åœ¨ profiler ä¸­æ·»åŠ  use_per_head_attention å‚æ•°ï¼‰
```

### 3. æ•°å€¼éªŒè¯

ç¡®ä¿ä¸¤ç§æ¨¡å¼äº§ç”Ÿç›¸åŒçš„è¾“å‡ºï¼š

```python
import torch
from experiments.models.sgx_bert_native import create_bert_base

# åˆ›å»ºä¸¤ä¸ªæ¨¡å‹
model_batched = create_bert_base(use_per_head_attention=False)
model_per_head = create_bert_base(use_per_head_attention=True)

# ä½¿ç”¨ç›¸åŒçš„éšæœºè¾“å…¥
torch.manual_seed(42)
input_ids = torch.randint(0, 30522, (1, 128))

# æ¯”è¾ƒè¾“å‡º
# ï¼ˆéœ€è¦åŠ è½½ç›¸åŒçš„æƒé‡ï¼‰
```

## ğŸ“ˆ æ€§èƒ½å½±å“

### æ‰¹é‡æ¨¡å¼

- **æ€§èƒ½**ï¼šä¸åŸå®ç°å®Œå…¨ç›¸åŒ
- **å†…å­˜**ï¼šä¸åŸå®ç°ç›¸åŒ
- **å±‚æ•°**ï¼šä¸åŸå®ç°ç›¸åŒ

### Per-head æ¨¡å¼

- **æ€§èƒ½**ï¼šçº¦ 5-10% çš„å¼€é”€ï¼ˆå¤šæ¬¡ kernel è°ƒç”¨ï¼‰
- **å†…å­˜**ï¼šå³°å€¼é™ä½ï¼ˆå¯é€å¤´å¤„ç†ï¼‰
- **å±‚æ•°**ï¼šå¢åŠ  10-15 å€ï¼ˆæ¯ä¸ªå¤´ç‹¬ç«‹ï¼‰

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **Per-head æ¨¡å¼ä¸‹å±‚æ•°å¤§å¹…å¢åŠ **
   - BERT-base: ä» ~150 å±‚å¢åŠ åˆ° ~1500+ å±‚
   - å¯èƒ½éœ€è¦è°ƒæ•´ profiling è„šæœ¬çš„ Enclave é‡ç½®é¢‘ç‡

2. **å†…å­˜ç®¡ç†**
   - Per-head æ¨¡å¼å¯ä»¥é™ä½å³°å€¼å†…å­˜
   - ä½†éœ€è¦æ›´å¤šçš„å±‚é—´é€šä¿¡

3. **å…¼å®¹æ€§**
   - ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯ç»§ç»­ä½¿ç”¨ï¼ˆé»˜è®¤æ‰¹é‡æ¨¡å¼ï¼‰
   - éœ€è¦ç»†ç²’åº¦åˆ†ææ—¶æ‰å¯ç”¨ per-head æ¨¡å¼

## ğŸš€ åç»­ä¼˜åŒ–æ–¹å‘

1. **æ·»åŠ  Concat å±‚æ”¯æŒ**
   - å½“å‰ per-head æ¨¡å¼çš„æ‹¼æ¥æ˜¯ç®€åŒ–å®ç°
   - å¯ä»¥æ·»åŠ ä¸“é—¨çš„ `SecretConcatLayer` æ¥æ­£ç¡®å¤„ç†

2. **åŠ¨æ€ head æ•°é‡**
   - æ”¯æŒè¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´å¤´æ•°

3. **Pruning æ”¯æŒ**
   - Per-head æ¨¡å¼ä¸‹å¯ä»¥å•ç‹¬ç§»é™¤æŸäº›å¤´

4. **æ€§èƒ½ä¼˜åŒ–**
   - ä¼˜åŒ– per-head æ¨¡å¼çš„å†…å­˜è®¿é—®æ¨¡å¼
   - å‡å°‘å±‚é—´é€šä¿¡å¼€é”€

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `python/layers/attention/README.md` - è¯¦ç»†ä½¿ç”¨æ–‡æ¡£
- `experiments/models/sgx_bert_native.py` - BERT å®ç°ç¤ºä¾‹
- `experiments/models/sgx_swin_native.py` - Swin ç‰¹æ®Šé€‚é…ç¤ºä¾‹

## âœ… éªŒæ”¶æ¸…å•

- [x] åˆ›å»ºé€šç”¨æ³¨æ„åŠ›æ¨¡å—
- [x] æ›´æ–° BERT æ¨¡å‹
- [x] æ›´æ–° ALBERT æ¨¡å‹
- [x] æ›´æ–° DistilBERT æ¨¡å‹
- [x] æ›´æ–° TinyBERT æ¨¡å‹
- [x] æ›´æ–° ViT æ¨¡å‹
- [x] æ›´æ–° Swin Transformer æ¨¡å‹
- [x] ç¼–å†™è¯¦ç»†æ–‡æ¡£
- [ ] è¿è¡Œæµ‹è¯•éªŒè¯
- [ ] æ€§èƒ½ profiling å¯¹æ¯”

## ğŸ‘¥ è´¡çŒ®è€…

æœ¬æ¬¡é‡æ„ç”± TAOISM é¡¹ç›®å›¢é˜Ÿå®Œæˆã€‚

---

**é‡æ„å®Œæˆæ—¥æœŸ**ï¼š2026-01-20
**ä»£ç è¡Œæ•°å˜åŒ–**ï¼š-990 è¡Œé‡å¤ä»£ç ï¼Œ+730 è¡Œé€šç”¨ä»£ç 
**å‡€å‡å°‘**ï¼š260 è¡Œ
**æ–°å¢åŠŸèƒ½**ï¼šPer-head ç»†ç²’åº¦åˆ†ææ¨¡å¼
